# MSDSP462_CV_Hair_Segmentation


This repository contains Jupyter Notebooks that implement various deep learning models for face segmentation, specifically focusing on **hair segmentation**. The project explores different architectures to segment hair regions from face images, using the CelebAMask-HQ dataset.

---

## 📂 Repository Structure

```
├── custom_cnn_Face_Image_Segmentation_v2.ipynb
├── custom_cnn_Face_Image_Segmentation_eval_v2.ipynb
├── deeplabv3_Face_Image_Segmentation.ipynb
├── Face_Image_Segmentation_v2.ipynb
├── Face_Image_Segmentation_eval_v2.ipynb
└── val_images/                # Validation samples for testing
```

---

## 📒 Notebooks Overview

- **custom_cnn_Face_Image_Segmentation_v2.ipynb**  
  Implements a custom CNN model for hair segmentation using an encoder-decoder architecture with batch normalization and ReLU activations.

- **custom_cnn_Face_Image_Segmentation_eval_v2.ipynb**  
  Evaluates the performance of the custom CNN model on validation and test datasets.

- **deeplabv3_Face_Image_Segmentation.ipynb**  
  Implements the DeepLabV3 model for face and hair segmentation. DeepLabV3 utilizes atrous convolutions and spatial pyramid pooling for high-quality segmentation.

- **Face_Image_Segmentation_v2.ipynb**  
  General notebook showcasing preprocessing, model training, and evaluation for different architectures including U-Net and DeepLabV3.

- **Face_Image_Segmentation_eval_v2.ipynb**  
  Evaluates trained models and visualizes predictions on validation datasets.

---

## 📂 Validation Samples

The folder `val_images/` contains sample validation images and masks used to evaluate and test the models. These help visualize how well the model generalizes to unseen data.

---

## 🗄️ Dataset

- **Dataset Used:** CelebAMask-HQ  
  - 30,000 high-resolution face images (512x512) with 19 labeled facial components, including hair, skin, and eyes.
  - This project focuses on **hair segmentation**, using a subset of 20,000 images.
  
You can download the dataset [here](https://github.com/switchablenorms/CelebAMask-HQ) and adjust paths for `IMAGE_DIR` and `MASK_DIR` in the notebooks.

---

## ⚙️ Requirements

- Python 3.7+
- TensorFlow 2.x
- OpenCV
- NumPy
- Matplotlib
- Scikit-learn

Install the dependencies:

```bash
pip install tensorflow opencv-python numpy matplotlib scikit-learn
```

---

## 🚀 Usage

1. **Download and Prepare Dataset:**  
   Download CelebAMask-HQ, extract it, and set up the paths (`IMAGE_DIR`, `MASK_DIR`).

2. **Run the Notebooks:**  
   Launch Jupyter Notebook or Google Colab and run the desired notebooks to:
   - Train a model  
   - Evaluate performance  
   - Perform inference on new images  

3. **Test on Validation Images:**  
   Use the `val_images/` folder to test the models on unseen data and visualize the segmentation results.

---

## 📊 Results

- Visualizations of training progress, loss curves, and evaluation metrics (accuracy, precision, recall) are included in each notebook.
- Segmentation masks generated by different models are compared for visual quality and accuracy.

---

## 💼 Business Problem & Use Cases

### Business Problem
- **Virtual Try-Ons** for spectacles, jewelry, hats, and hairstyles.
- **Face Editing & Generation** for digital avatars and new looks.
- **Healthcare Applications** like facial expression analysis to detect neurological conditions.

### Use Cases
- E-commerce (virtual try-ons)
- AR/VR experiences
- Gaming (customizable avatars)
- Medical imaging and diagnostics

---

## ✅ Progress

- Successfully trained custom CNN and DeepLabV3 models.
- Achieved improved accuracy and finer segmentation with DeepLabV3.
- Integrated validation testing with `val_images` to evaluate model robustness.

---

## ❗ Roadblocks

- Large dataset size causing longer training times.
- High computational requirements; multiple notebook crashes due to memory issues.
- Optimization needed for real-time inference and deployment.

---

## 🤝 Contributing

Contributions are welcome!  
Feel free to open issues or submit pull requests to suggest improvements or report bugs.

---

## 📝 License

This project is licensed under the MIT License.

---

